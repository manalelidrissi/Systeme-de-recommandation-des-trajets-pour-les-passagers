{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stage2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFaz8SM3VSON",
        "colab_type": "code",
        "outputId": "2eefbc45-000f-4249-baf3-54239dada69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/data-collection-from-transportation-platforms-morocco"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data-collection-from-transportation-platforms-morocco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAOUEjzoVNkw",
        "colab_type": "code",
        "outputId": "170ae78d-db6d-484b-e777-8e5cc7c75f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input,Dense, concatenate,Dropout,BatchNormalization,Activation,Flatten,Add, Conv2D, Dot, dot\n",
        "from keras.layers import RepeatVector, merge, Subtract, Lambda, Multiply, Embedding, Concatenate, Reshape, DepthwiseConv2D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Layer\n",
        "from sklearn.metrics import roc_auc_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAeqcX1UVRE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = \"rating\"\n",
        "\n",
        "CATEGORICAL_COLUMNS = [\n",
        "    'from', 'to' ,'session','gender', 'profission','dayofweek'\n",
        "]\n",
        "\n",
        "CONTINUOUS_COLUMNS = [\n",
        "    'distance','duration', 'price','age'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtdaVxEAW37J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing():\n",
        "    data = pd.read_csv('dataset.csv',delimiter =';')\n",
        "    print(data.shape)\n",
        "    # Convert the position of each starting city to a number \n",
        "    a = data.groupby(['from_lat','from_lan']).size()\n",
        "    from_dict = {}\n",
        "    for i,row in enumerate(a.iteritems()):\n",
        "        from_dict[row[0]] = i\n",
        "    f = data[['from_lat','from_lan']]\n",
        "    from_col = []\n",
        "    for a,b in f.iterrows():\n",
        "        from_col.append(from_dict[(b['from_lat'],b['from_lan'])])\n",
        "    data['from'] = from_col\n",
        "    data = data.drop(['from_lat','from_lan'],axis = 1)\n",
        "    # Convert the position of each arrival city to a number \n",
        "    a = data.groupby(['to_lat','to_lan']).size()\n",
        "    from_dict = {}\n",
        "    for i,row in enumerate(a.iteritems()):\n",
        "        from_dict[row[0]] = i\n",
        "    f = data[['to_lat','to_lan']]\n",
        "    from_col = []\n",
        "    for a,b in f.iterrows():\n",
        "        from_col.append(from_dict[(b['to_lat'],b['to_lan'])])\n",
        "    data['to'] = from_col\n",
        "    data = data.drop(['to_lat','to_lan'],axis = 1)\n",
        "    \n",
        "    for c in CATEGORICAL_COLUMNS:\n",
        "        le = LabelEncoder()\n",
        "        data[c] = le.fit_transform(data[c])\n",
        "    # train,validation,test split\n",
        "    train=data.sample(frac=0.8,random_state=200)\n",
        "    val = data.drop(train.index).sample(frac = 0.5,random_state=200)\n",
        "    test=data.drop(train.index).drop(val.index)\n",
        "    \n",
        "    y_train = train['rating'].values\n",
        "    y_val = val['rating'].values\n",
        "    y_test = test['rating'].values\n",
        "    x_train = train.drop(['rating'],axis=1)\n",
        "    x_val = val.drop(['rating'],axis=1)\n",
        "    x_test = test.drop(['rating'],axis=1)\n",
        "    x_train_categ = np.array(x_train[CATEGORICAL_COLUMNS]) \n",
        "    x_val_categ = np.array(x_val[CATEGORICAL_COLUMNS])\n",
        "    x_test_categ = np.array(x_test[CATEGORICAL_COLUMNS])\n",
        "    x_train_conti = np.array(x_train[CONTINUOUS_COLUMNS], dtype='float64') \n",
        "    x_val_conti = np.array(x_val[CONTINUOUS_COLUMNS], dtype='float64')\n",
        "    x_test_conti = np.array(x_test[CONTINUOUS_COLUMNS], dtype='float64')\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    x_train_conti = scaler.fit_transform(x_train_conti)\n",
        "    x_val_conti = scaler.transform(x_val_conti)    \n",
        "    x_test_conti = scaler.transform(x_test_conti)\n",
        "    return [x_train, y_train, x_val, y_val, x_test, y_test, x_train_categ, x_val_categ, x_test_categ, x_train_conti,x_val_conti, x_test_conti, data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5fUWbCW6jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class xDeepMF:\n",
        "  def __init__(self,hidden_layer, conv_layer, embed_dim):\n",
        "    self.hidden_layer= hidden_layer # List of sizes of the hidden layers\n",
        "    self.conv_layer = conv_layer    # List of sizes of the conv layers\n",
        "    self.embed_dim = embed_dim      # size of embedding vectors\n",
        "    self.model = None \n",
        "    x_train, y_train, x_val, y_val, x_test, y_test, x_train_categ,x_val_categ, x_test_categ, x_train_conti,x_val_conti, x_test_conti, data = preprocessing()\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train / 5\n",
        "    self.x_val = x_val\n",
        "    self.y_val = y_val / 5\n",
        "    self.x_test = x_test\n",
        "    self.y_test = y_test / 5\n",
        "    self.x_train_categ = x_train_categ \n",
        "    self.x_val_categ = x_val_categ \n",
        "    self.x_test_categ = x_test_categ \n",
        "    self.x_train_conti = x_train_conti \n",
        "    self.x_test_conti = x_test_conti \n",
        "    self.x_val_conti = x_val_conti \n",
        "    self.all_data = data\n",
        "\n",
        "  def create_model(self,optimizer = \"adam\",loss=\"binary_crossentropy\"):\n",
        "    cat_input = []\n",
        "    cat_output = []\n",
        "    for i in range(len(CATEGORICAL_COLUMNS)):\n",
        "      input = Input(shape= (1,))\n",
        "      cat_input.append(input)\n",
        "      dim = len(np.unique(self.all_data[CATEGORICAL_COLUMNS[i]]))\n",
        "      emb = Embedding(dim, self.embed_dim, input_length =1 ,trainable = True)(input)\n",
        "      cat_output.append(emb)\n",
        "\n",
        "    cat_output = Concatenate(axis=1)(cat_output)\n",
        "\n",
        "    first_order = Flatten()(cat_output)\n",
        "    first_order = Dense(1, activation='linear')(first_order)\n",
        "\n",
        "    #cin_output = CIN(cat_output, self.conv_layer)\n",
        "\n",
        "    dense_input = Input(shape = (len(CONTINUOUS_COLUMNS), ))\n",
        "\n",
        "    dnn_input = Concatenate(axis=1)([Flatten()(cat_output), dense_input])\n",
        "\n",
        "    dnn_output = dnn_input \n",
        "    for layer in self.hidden_layer:\n",
        "      dnn_output  = BatchNormalization()(dnn_output)\n",
        "      dnn_output  = Dense(layer, activation='relu')(dnn_output)\n",
        "      dnn_output  = Dropout(0.2)(dnn_output)\n",
        "    dnn_output = Dense(1, activation='linear')(dnn_output)\n",
        "    output  = Add()([first_order, dnn_output])\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    model = Model(inputs = cat_input + [dense_input], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=optimizer,loss=loss,metrics=[\"accuracy\"])\n",
        "    self.model = model\n",
        "  \n",
        "  def train(self,batch_size = 64,epochs = 10):\n",
        "    train_data = [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])] + [self.x_train_conti] \n",
        "    val_data = [self.x_val_categ[:, i] for i in range(self.x_train_categ.shape[1])] + [self.x_val_conti]\n",
        "    self.model.fit(train_data,self.y_train, validation_data=(val_data,self.y_val),epochs = epochs,batch_size=batch_size)\n",
        "  \n",
        "  def evaluate(self):\n",
        "    input_data = [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])] + [self.x_test_conti] \n",
        "    loss, acc = self.model.evaluate(input_data, self.y_test)\n",
        "    print(f'test_loss: {loss} - test_acc: {acc}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqvPyJ2owWda",
        "colab_type": "code",
        "outputId": "6e5da427-6057-46ee-f7dd-ca069ef1817b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xdeepmf = xDeepMF(hidden_layer=[512,256,128,64],conv_layer=[25, 20, 20],embed_dim=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2357500, 18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpxBTD_xJWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xdeepmf.create_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ27zJWO4Rss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CIN(x, conv_layer):\n",
        "    #print('debug :',x.shape)\n",
        "    cat_output_expand = Lambda(lambda x: K.expand_dims(x, axis = 2))(x)\n",
        "    #print('debug :',cat_output_expand.shape)\n",
        "    # shape: -1, 1, feature_size, dim \n",
        "    x_0 = Lambda(lambda x: K.permute_dimensions(x, (0,3,2,1)))(cat_output_expand)\n",
        "    x_next = x_0\n",
        "\n",
        "    cin_output = []\n",
        "    for layer in conv_layer:\n",
        "        z_0 = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2,3)))([x_0, x])\n",
        "        z_1 = Lambda(lambda x: K.permute_dimensions(x, (0,2,3,1)))(z_0)\n",
        "\n",
        "        x_next_list = []\n",
        "        pooling_output_list = []\n",
        "        for index in range(layer):\n",
        "            output = DepthwiseConv2D((int(z_1.shape[1]), int(z_1.shape[2])))(z_1)\n",
        "            #output = Conv2D(int(cat_output_expand.shape[-1]), (int(z_1.shape[1]), int(z_1.shape[2])) )(z_1)\n",
        "            output = Lambda(lambda x: K.squeeze(x, 2))(output)\n",
        "            pooling_output = Lambda(lambda x: K.sum(output, axis = 2))(output)\n",
        "            pooling_output_list.append(pooling_output)\n",
        "            x_next_list.append(output)\n",
        "        x_next = Concatenate(axis = 1)(x_next_list)\n",
        "        x_next = Lambda(lambda x: K.expand_dims(x, axis = 2))(x_next)\n",
        "\n",
        "        x_pooling = Concatenate(axis = 1)(pooling_output_list)\n",
        "        cin_output.append(x_pooling)\n",
        "\n",
        "    cin_output =  Concatenate(axis = 1)(cin_output)\n",
        "    cin_output = Dense(1, activation='linear')(cin_output)\n",
        "    return cin_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvUSSqfyrWq",
        "colab_type": "code",
        "outputId": "56591920-7234-4779-bbab-5e3d3b6a5a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "xdeepmf.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1886000 samples, validate on 235750 samples\n",
            "Epoch 1/10\n",
            " 715840/1886000 [==========>...................] - ETA: 3:20 - loss: 0.5375 - accuracy: 0.1751"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4aUXHes32hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}